# S3

Used for ”infinitely scaling” storage

### Buckets

Store objects (files) in “buckets” (directories)\
Buckets must have a globally unique name (across all regions all accounts), defined at the region level

#### Naming Convention

* No uppercase, No underscore, 3-63 characters long, Not an IP, start w lowercase or number
* NOT start with the prefix xn-- or end with the suffix -s3alias

### Objects

<div align="left"><figure><img src="../../.gitbook/assets/image (188).png" alt="" width="356"><figcaption></figcaption></figure></div>

* Max. Object Size is 5TB, if more than 5GB use multi-part upload
* Metadata (list of text key / value pairs)
* Tags (Unicode key / value pair – up to 10) – useful for security / lifecycle
* Version ID if enabled

### Security

* IAM Policies – which API calls should be allowed for a specific user from IAM
* Bucket Policies – bucket wide rules from the S3 console - allows cross account
* Object Access Control List (ACL) – finer grain (can be disabled)
* Bucket Access Control List (ACL) – less common (can be disabled)

**IAM principal can access an S3 object if IAM permissions ALLOW it OR the resource policy ALLOWS it  AND there’s no explicit DENY**

<div align="left"><figure><img src="../../.gitbook/assets/image (189).png" alt="" width="563"><figcaption></figcaption></figure></div>

**Static Website Hosting**\
[http://bucket-name.s3-website-aws-region.amazonaws.com](http://bucket-name.s3-website-aws-region.amazonaws.com) or [http://bucket-name.s3-website.aws-region.amazonaws.com](http://bucket-name.s3-website.aws-region.amazonaws.com) peep the dot and hyphen before s3

### Versioning

Enabled at bucket level, easy to restore to and manage resources

Files created before versioning is enabled is set to null

### Replication

Must have versioning enabled\
Cross-Region Replication (CRR) (compliance, lower latency access) and Same-Region Replication (SRR) (log aggregation, live replication bw production and test)

Copying is asynchronous and proper IAM perms needed

Only new objects are replicated after enabling replication\
replicate existing objects using S3 Batch Replication (existing objects and objects that failed replication)

For delete operations, Can replicate delete markers from source to target (optional), versioned ones can't be replicated tho

### Storage Classes

**General Purpose** - frequently accessed data, low latency and high throughput

**Infrequent Access** - less accessed but rapid when needed\
S3 Standard-IA: Disaster Recovery, backups\
S3 One Zone-IA: Store secondary backup copies of on-premises data

**Glacier** - for archiving/backup\
Glacier Instant Retrieval - Millisecond retrieval, minimum 90 days\
Glacier Flexible Retrieval - Expedited (1 to 5 minutes), Standard (3 to 5 hours), Bulk (5 to 12 hours) – free, minimum 90 days\
Glacier Deep Archive - Standard (12 hours), Bulk (48 hours), minimum 180 days

Intelligent-tiering auto switch class based on usage, pay small monthly fee

<figure><img src="../../.gitbook/assets/image (190).png" alt=""><figcaption></figcaption></figure>

## Advanced

**To move between storage classes, downwards in terms of read speed , automate w lifecycle rules**\
Perform transition and expiration actions, for specific prefixes and tags

**Storage Class Analysis** - Recommendations for Standard and Standard IA when to transition, reports every 24 to 48 hours

Requester Pays - Normally owner pay for storage and networking cost, here requester pays for networking (useful for sharing large databases with others, can't be anonymous)

<div align="left"><figure><img src="../../.gitbook/assets/image (187).png" alt="" width="563"><figcaption></figcaption></figure></div>

Can use **EventBridge** with advance filters JSON options, multiple dests and archive, replay and other cool features this thing has to offer

#### Performance

Upto 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in a bucket.\
No limit on the number of prefixes in a bucket

bucket/folder1/sub1/file => /folder1/sub1/\
bucket/1/file => /1/

**Improve Upload Speeds**

Multi-part uploads - recommended > 100mb and must > 5gb\
S3 Transfer Acceleration - transferring file to an AWS edge location which will forward the data to the S3 bucket in the target region

**Improve Read Speeds**

Byte-Range Fetches - parallelize GETs by requesting specific byte ranges OR retrieve only partial data (for example the head of a file)

S3 Select & Glacier Select - Perform server side filtering (rows and columns)&#x20;

### Batch Operations

<div align="left"><figure><img src="../../.gitbook/assets/image (66).png" alt="" width="563"><figcaption><p>ENCRYPT UNENCRYPTED STUFF W THIS WATCHHH</p></figcaption></figure></div>
